{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Wine Recommendations**\n",
    "\n",
    "**Evan Slack, Dylan Wagman, Ryan Maiman**\n",
    "\n",
    "Final Report\n",
    "\n",
    "ME 193: Data Analytics, Spring 2021\n",
    "\n",
    "Professor Howard Hamilton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With hundreds of thousands of unique wines in existence, it can be daunting to identify which wine a user will enjoy. Wine connoisseurs want to know what type of wine they will like based on their current preferences. Our solution parses over 130,000 wine reviews scraped from WineEnthusiast.com to offer a wine recommendation based on user input. The data is used to determine the most important characteristics and build a model that groups wine by similarities. To accurately create this recommendation system, we applied a Word2Vec algorithm through Amazon SageMaker, a cloud computing and machine learning service. SageMaker was used to establish a model and implement a word embeddings algorithm to the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How It Works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recomendation systems have become increasingly prevalent in digital services. Therefore, our team felt it would be beneficial to learn the ins and outs of creating these types of systems. A wine reccomendation system fit perfectly with both our professional and personal interests. Thus, this notebook was born.\n",
    "\n",
    "Our algorithm takes in a user input wine. The user can select a type of wine in the dataset from a drop down menu. From here, the wine descriptors are taken into account by using a word2vec algorithm, explained in more detail under the Model Description section. These descriptors are then compared to the other wine in the dataset and an output of the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Related Works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recomendation systems are not a new idea, however, many of current systems use a collaborative filtering algorithm. Collaborative filtering works by creating a matrix of users and their preferences. A separate user then inputs their preferences, which is compared to the preferences of the existing users. If a similar preference is found, recommendations are made based on the matching user's other preferences. \n",
    "\n",
    "Examples of some recommendation systems that use collaborative filtering in some way are Spotify, Amazon, and Netflix. These systems have millions of users and thousands to millions of items. They can create and update their user preference matrices in real time to provide users with accurate reccomendations. Likely, these companies are using collaborative filtering in conjunction with other algorithms to provide the best suggestions for their users. \n",
    "\n",
    "Initially we wanted to pursue this sort of method for our reccomendation systems but after analyzing our dataset, realized that it would not be the most effective due to a lack of significant users. Therefore, the user x preference matrix was too sparse. We instead choose to use word embeddings to find similar wine descriptors. \n",
    "\n",
    "Word embeddings can be found in a lot of the technology we interact with on a daily basis. When writing emails or text messages, software will commonly provide next word (autofill) suggestions. These predictions are typically based on a Continuous Bag-of-Words implementation of Word2Vec, a word embeddings algorithm discussed later on in the algorithm section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 130,000+ reviews from WineEnthusiast.com. It was scraped and able to be downloaded from kaggle. Once downloaded, the data was consolidated, and cleaned. The wine characteristics include descriptions written in prose, prices, wineries, and varieties. An additional wine feature, 'name', was created by combining the winery and wine variety. An example of the dataset and the full list of characteristics can be seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset URL: https://www.kaggle.com/zynicide/wine-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![WineList](Images/WineList.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our recommendation system, the reviews written in prose had to be standardized. To do this, the following steps were completed:\n",
    "1. A large text corpus with all of the reviews was created\n",
    "2. The corpus was tokenized to remove stop words\n",
    "3. Similar words were grouped together\n",
    "4. Uninformative descriptors were removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word embeddings** is a natural language processing technique in which words from a body of text are mapped to a vector space in which words with similar meanings are closer together. The problem is that most machine learning techniques are unable to process strings of letters. These algorithms want to recieve numerical inputs instead. Therefore, in order to perform numerical experiments on bodies of text, the words must be converted into a vector space representation. Once in this space, a simple consine distance metric can be used \n",
    "\n",
    "There are several methods for mapping words to this numerical space; We chose to work with an unsupervised learning algorithm called **Word2Vec**. This algorithm was created by a machine learning team at Google in 2013 and is capable of capturing semantic relationships between words from a large dataset. There are two distinct types of Word2Vec, Contextual Bag-Of-Words (CBOW) and **Skip-Gram with Negative Sampling** (SGNS). CBOW aims to takes a context input of previous words and tries to predict the following word where SGNS takes an input of one word and tries to predict the context (similar words). As we are trying to take a descriptor from a wine review and predict similar descriptors, we are more concerned with the SGNS architecture.\n",
    "\n",
    "![SkipGram](Images/SkipGram.png)\n",
    "\n",
    "The model is built with a 3 layer neural network as seen in the image above. Essentially each input computes a dot product with the weight matrix of the hidden layer, then the outer layer computes a dot product with the output of the middle layer and the weight matrix of the outer layer. Finally, a softmax activtion function is applied to this output to determine the probability of words appearing in the context of the input word. \n",
    "\n",
    "The softmax probability function is as follows: \n",
    "\n",
    "$$ s(z) = \\frac{e^{z_i}}{\\sum_{i=1}^{K}e^{z_j}} $$\n",
    "\n",
    "where \n",
    "$$e^z = \\text{output layer elements}$$\n",
    "$$K = \\text{# output layer elements}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to begin creating the recommendation system, we had to first import the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# import top level modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, NearestNeighbors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was pulled from the downloaded kaggle file and any wines with too many descriptors were removed from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# load in data generated from blazing text algorithm \n",
    "# this includes original dataset with wine details, and the individual wine embedding vectors \n",
    "full_wine_df = pd.read_pickle(os.path.join(\"Data\", \"full_wine_df.pkl\"))\n",
    "full_wine_df_test = pd.read_pickle(os.path.join(\"Data\", \"full_wine_df_test.pkl\"))\n",
    "df_reds = pd.read_pickle(os.path.join(\"Data\", \"df_reds.pkl\"))\n",
    "full_wine_df.head()\n",
    "df_reds.head()\n",
    "\n",
    "\n",
    "#use only well described wines, this is wines with more than a threshold of descriptors\n",
    "full_wine_df = full_wine_df.loc[full_wine_df['descriptor_count'] > 5]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering was applied to get a better understanding of the dataset and the relationship between the wines. A cosine metric because it was the most succinct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# make input X vector for Nearest Neighbors\n",
    "X_in = pd.DataFrame(full_wine_df['review_vector'].values.tolist())\n",
    "X_in.dropna(inplace=True)\n",
    "\n",
    "# build KNN model with training data\n",
    "knn = NearestNeighbors(n_neighbors=10, algorithm= 'brute', metric='cosine')\n",
    "model_knn = knn.fit(X_in)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# input the desired wine name or generate a random sample from test set\n",
    "name_test = full_wine_df_test.sample().name.iloc[0]\n",
    "\n",
    "# first attempts to search training set otherwise looks in test set for the input wine\n",
    "# finds neighbours  of desired wine\n",
    "try:\n",
    "    #print(\"train\")\n",
    "    wine_test_vector = full_wine_df.loc[full_wine_df['name'] == name_test]['review_vector'].values.tolist()     \n",
    "    distance, indice = model_knn.kneighbors(wine_test_vector, n_neighbors=4)\n",
    "    distance_list = distance[0].tolist()[1:]\n",
    "    indice_list = indice[0].tolist()[1:]\n",
    "name_test = \"Ponzi Pinot Noir\"\n",
    "\n",
    "wine_test_vector = full_wine_df.loc[full_wine_df['name'] == name_test]['review_vector'].values.tolist()\n",
    "distance, indice = model_knn.kneighbors(wine_test_vector, n_neighbors=4)\n",
    "distance_list = distance[0].tolist()[1:]\n",
    "indice_list = indice[0].tolist()[1:]\n",
    "\n",
    "main_wine = full_wine_df.loc[full_wine_df['name'] == name_test]\n",
    "\n",
    "    print('Similar to:', name_test)\n",
    "    print('The input wine is:', list(main_wine['descriptors'])[0])\n",
    "    print('_________')\n",
    "    n = 1\n",
    "    for d, i in zip(distance_list, indice_list):\n",
    "        wine_name = full_wine_df['name'][i]\n",
    "        wine_descriptors = full_wine_df['descriptors'][i]\n",
    "        wine_price = df_reds.loc[df_reds['name'] == wine_name]['price'].values.tolist()[0]\n",
    "        wine_country = df_reds.loc[df_reds['name'] == wine_name]['country'].values.tolist()[0]\n",
    "        wine_points = df_reds.loc[df_reds['name'] == wine_name]['points'].values.tolist()[0]\n",
    "        #print('Suggestion', str(n), ':', wine_name, 'distance of', \"{:.3f}\".format(d))\n",
    "        print('Suggestion', str(n), ':', wine_name)\n",
    "        #print('and descriptors:', wine_descriptors)\n",
    "        #print('')\n",
    "        print('Price ($):', wine_price)\n",
    "        print('Country:', wine_country)\n",
    "        print('Points:', wine_points)\n",
    "        print('')\n",
    "        n+=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "except:\n",
    "    #print(\"test\")\n",
    "    wine_test_vector = full_wine_df_test.loc[full_wine_df_test['name'] == name_test]['review_vector'].values.tolist()     \n",
    "    distance, indice = model_knn.kneighbors(wine_test_vector, n_neighbors=4)\n",
    "    distance_list = distance[0].tolist()[1:]\n",
    "    indice_list = indice[0].tolist()[1:]\n",
    "\n",
    "    main_wine = full_wine_df_test.loc[full_wine_df_test['name'] == name_test]\n",
    "\n",
    "    print('Similar to:', name_test)\n",
    "    print('The input wine is:', list(main_wine['descriptors'])[0])\n",
    "    print('_________')\n",
    "    n = 1\n",
    "    for d, i in zip(distance_list, indice_list):\n",
    "        wine_name = full_wine_df_test['name'][i]\n",
    "        wine_descriptors = full_wine_df_test['descriptors'][i]\n",
    "        wine_price = df_reds.loc[df_reds['name'] == wine_name]['price'].values.tolist()[0]\n",
    "        wine_country = df_reds.loc[df_reds['name'] == wine_name]['country'].values.tolist()[0]\n",
    "        wine_points = df_reds.loc[df_reds['name'] == wine_name]['points'].values.tolist()[0]\n",
    "        #print('Suggestion', str(n), ':', wine_name, 'distance of', \"{:.3f}\".format(d))\n",
    "        print('Suggestion', str(n), ':', wine_name)\n",
    "        #print('and descriptors:', wine_descriptors)\n",
    "        #print('')\n",
    "        print('Price ($):', wine_price)\n",
    "        print('Country:', wine_country)\n",
    "        print('Points:', wine_points)\n",
    "        print('')\n",
    "        n+=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User Interface**\n",
    "\n",
    "The GUI uses the ipywidgets within Jupyter Notebook. We chose to use the combobox feature because it allows the user to choose from a drop down menu, but they are also able to type into the interface to narrow down their selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "name = df_reds['name']\n",
    "\n",
    "all_names = pd.unique(name).tolist()\n",
    "\n",
    "input_names = widgets.Combobox(\n",
    "    placeholder='Choose a wine',\n",
    "    options= all_names,\n",
    "    description='',\n",
    "    ensure_option=True,\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "print(\"Enter Wine Name\")\n",
    "display(input_names)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendation Strength**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cosine distance metric was used to determine the strength of the recommendation. A wine with a distance metric closer to zero is a better suggestion than a wine with a larger distance metric. The output below shows a potential recommendation result. In this case, the Domaine Saint Andrieu Rhone-style Red Blend is quantitatively the best suggestion. This Red Blend can also be classified as the best qualitative suggestion based on the descriptors. Stonier Pinot Noir and the Red Blend have the most similar flavors, such as fruit/cherry/berry and earth/wood/root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![StonierPinot](Images/StonierPinot.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User Interface**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user is able to enter a type of wine they have enjoyed from a drop down menu with all of the wines in the dataset. This drop down menu is quite large, however, it shrinks once the user begins to type. The interface was greated with Jupyter's interactive ipywidgets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GUI](Images/GUI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user output is simpilar than the output shown above in the Stonier Pinot Noir experiment. We thought that the most valuable information for the users' wine recommendations would be the wine names, prices, countries, and wine points. Thus, the user output interface looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/UserOutput.jpeg\" alt=\"UserOutput\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evan**: Created bar and scatterplots, clustered data using kmeans\n",
    "\n",
    "**Dylan**: Scraped and standardized the data from the Vivino website, created a random forest tree\n",
    "\n",
    "**Ryan**: Project proposal and delivery, project progress report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Related Work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R. Schuring, “RoboSomm Chapter 3: Wine Embeddings and a Wine Recommender,” Medium, 28-Dec-2019. [Online]. Available: https://towardsdatascience.com/robosomm-chapter-3-wine-embeddings-and-a-wine-recommender-9fc678f1041e. [Accessed: 11-May-2021]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm Information**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Gensim: topic modelling for humans,” models.word2vec – Word2vec embeddings - gensim, 29-Apr-2021. [Online]. Available: https://radimrehurek.com/gensim/models/word2vec.html. [Accessed: 11-May-2021].\n",
    "\n",
    "D. Radečić, “Softmax Activation Function Explained,” Medium, 18-Jun-2020. [Online]. Available: https://towardsdatascience.com/softmax-activation-function-explained-a7e1bc3ad60. [Accessed: 11-May-2021].\n",
    "\n",
    "N. S. S. I. am a perpetual, “Understanding Word Embeddings: From Word2Vec to Count Vectors,” Analytics Vidhya, 19-Oct-2020. [Online]. Available: https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/. [Accessed: 11-May-2021].\n",
    "\n",
    "S. Engdahl, “Blogs,” Amazon, 2008. [Online]. Available: https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-blazingtext-parallelizing-word2vec-on-multiple-cpus-or-gpus/. [Accessed: 11-May-2021].\n",
    "\n",
    "S. Doshi, “Skip-Gram: NLP context words prediction algorithm,” Medium, 17-Mar-2019. [Online]. Available: https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c. [Accessed: 11-May-2021]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code and Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn\n",
    ">Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
